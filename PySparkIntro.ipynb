{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-2.3.0.tar.gz (211.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 211.9MB 1.5kB/s eta 0:00:01 1% |▋                               | 4.1MB 6.6MB/s eta 0:00:32    7% |██▎                             | 15.3MB 11.2MB/s eta 0:00:18 14.7MB/s eta 0:00:12    21% |███████                         | 45.8MB 9.2MB/s eta 0:00:19    23% |███████▌                        | 49.6MB 6.4MB/s eta 0:00:26    24% |███████▉                        | 51.9MB 14.7MB/s eta 0:00:11/s eta 0:00:11[K    33% |██████████▊                     | 71.1MB 7.5MB/s eta 0:00:19:00:11    38% |████████████▍                   | 82.0MB 15.7MB/s eta 0:00:09            | 97.5MB 7.7MB/s eta 0:00:15   | 101.7MB 14.3MB/s eta 0:00:08MB/s eta 0:00:08��███████▉               | 111.8MB 19.2MB/s eta 0:00:06  54% |█████████████████▌              | 116.0MB 17.1MB/s eta 0:00:06    55% |█████████████████▉              | 118.2MB 12.2MB/s eta 0:00:08    56% |██████████████████              | 118.8MB 7.6MB/s eta 0:00:13   | 124.7MB 28.3MB/s eta 0:00:04�████             | 125.2MB 12.6MB/s eta 0:00:07    60% |███████████████████▎            | 127.7MB 7.7MB/s eta 0:00:11    61% |███████████████████▊            | 130.5MB 21.0MB/s eta 0:00:04�█████████            | 132.0MB 17.0MB/s eta 0:00:05███████████████▎           | 134.1MB 3.0MB/s eta 0:00:27|████████████████████▌           | 135.6MB 13.4MB/s eta 0:00:06��█████████████▊           | 137.0MB 16.2MB/s eta 0:00:05    66% |█████████████████████▏          | 140.4MB 8.1MB/s eta 0:00:09    66% |█████████████████████▎          | 140.7MB 5.8MB/s eta 0:00:13    68% |█████████████████████▉          | 144.4MB 26.2MB/s eta 0:00:03��██████████████████▋         | 149.8MB 16.2MB/s eta 0:00:04█████▊         | 150.3MB 20.5MB/s eta 0:00:03��███████████▎        | 154.3MB 8.3MB/s eta 0:00:07�███████████████▋        | 156.6MB 19.0MB/s eta 0:00:03��█████████████████        | 158.8MB 23.9MB/s eta 0:00:03  | 159.5MB 4.9MB/s eta 0:00:11    76% |████████████████████████▋       | 162.7MB 20.2MB/s eta 0:00:03�███████████████████████▊       | 163.6MB 6.4MB/s eta 0:00:08  | 168.3MB 20.9MB/s eta 0:00:03  | 170.9MB 14.7MB/s eta 0:00:03███████▌     | 175.3MB 8.4MB/s eta 0:00:05�██████     | 178.8MB 11.2MB/s eta 0:00:03�██████████████████▍    | 181.7MB 29.6MB/s eta 0:00:02��███████████████████▋    | 182.9MB 13.5MB/s eta 0:00:03██████████████████▉    | 184.3MB 12.7MB/s eta 0:00:03% |████████████████████████████▌   | 188.4MB 12.6MB/s eta 0:00:02�███████████████████████████▊   | 190.0MB 13.2MB/s eta 0:00:02��▎  | 193.7MB 9.6MB/s eta 0:00:029MB 11.9MB/s eta 0:00:02MB/s eta 0:00:02�██▋ | 202.6MB 4.5MB/s eta 0:00:03███████▊ | 203.4MB 13.0MB/s eta 0:00:01�██████ | 204.6MB 13.2MB/s eta 0:00:01�| 206.3MB 22.4MB/s eta 0:00:01�| 208.9MB 22.5MB/s eta 0:00:01�██████████████████| 211.2MB 21.5MB/s eta 0:00:019% |████████████████████████████████| 211.8MB 9.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.6 (from pyspark)\n",
      "  Downloading py4j-0.10.6-py2.py3-none-any.whl (189kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Running setup.py bdist_wheel for pyspark ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/4f/39/ba/b4cb0280c568ed31b63dcfa0c6275f2ffe225eeff95ba198d6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.6 pyspark-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download requisite data to make available in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-25 02:22:47 URL:https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2017-06.csv [87477546/87477546] -> \"green_tripdata_2017-06.csv.1\" [1]\n",
      "2018-01-25 02:22:47 URL:https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv [12322/12322] -> \"taxi_zone_lookup.csv\" [1]\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget --no-verbose https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2017-06.csv\n",
    "wget -O taxi_zone_lookup.csv --no-verbose https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register special SQL magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "# https://github.com/LucaCanali/Miscellaneous/blob/master/Pyspark_SQL_Magic_Jupyter/IPython_Pyspark_SQL_Magic.ipynb\n",
    "# Configuration parameters\n",
    "max_show_lines = 50         # Limit on the number of lines to show with %sql_show and %sql_display\n",
    "detailed_explain = True     # Set to False if you want to see only the physical plan when running explain\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def sql(line, cell=None):\n",
    "    \"Return a Spark DataFrame for lazy evaluation of the sql. Use: %sql or %%sql\"\n",
    "    val = cell if cell is not None else line \n",
    "    return spark.sql(val)\n",
    "\n",
    "@register_line_cell_magic\n",
    "def sql_show(line, cell=None):\n",
    "    \"Execute sql and show the first max_show_lines lines. Use: %sql_show or %%sql_show\"\n",
    "    val = cell if cell is not None else line \n",
    "    return spark.sql(val).show(max_show_lines) \n",
    "\n",
    "@register_line_cell_magic\n",
    "def sql_display(line, cell=None):\n",
    "    \"\"\"Execute sql and convert results to Pandas DataFrame for pretty display or further processing.\n",
    "    Use: %sql_display or %%sql_display\"\"\"\n",
    "    val = cell if cell is not None else line \n",
    "    return spark.sql(val).limit(max_show_lines).toPandas() \n",
    "\n",
    "@register_line_cell_magic\n",
    "def sql_explain(line, cell=None):\n",
    "    \"Display the execution plan of the sql. Use: %sql_explain or %%sql_explain\"\n",
    "    val = cell if cell is not None else line \n",
    "    return spark.sql(val).explain(detailed_explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a SparkSession, the gateway to everything in Spark (2.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import time\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.36.57.120:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6f537e9c88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(name=\"PySpark Intro\").master(\"local[*]\").getOrCreate().newSession()\n",
    "# .config(\"spark.jars\", \"hadoop-aws-2.7.3.jar\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://10.36.57.120:4040'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext._jsc.addJar(\"hadoop-aws-2.7.3.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark DataFrame from CSV with header and inferred Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 12.3 ms, total: 12.3 ms\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "green_trips = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"green_tripdata_2017-06.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_trips.createOrReplaceTempView(\"green_trips\")\n",
    "revenue_by_hour = spark.sql(\"\"\"\n",
    "SELECT hour(lpep_pickup_datetime), SUM(total_amount) AS total\n",
    "FROM green_trips\n",
    "GROUP BY hour(lpep_pickup_datetime)\n",
    "ORDER BY hour(lpep_pickup_datetime) ASC\"\"\")\n",
    "revenue_by_hour.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caching](Screen Shot 2018-01-24 at 12.39.47 PM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [hour(lpep_pickup_datetime)#54, total#52]\n",
      "   +- InMemoryRelation [hour(lpep_pickup_datetime)#54, total#52], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *Sort [hour(lpep_pickup_datetime)#54 ASC NULLS FIRST], true, 0\n",
      "            +- Exchange rangepartitioning(hour(lpep_pickup_datetime)#54 ASC NULLS FIRST, 200)\n",
      "               +- *HashAggregate(keys=[hour(lpep_pickup_datetime#13, Some(Etc/UTC))#59], functions=[sum(total_amount#28)])\n",
      "                  +- Exchange hashpartitioning(hour(lpep_pickup_datetime#13, Some(Etc/UTC))#59, 200)\n",
      "                     +- *HashAggregate(keys=[hour(lpep_pickup_datetime#13, Some(Etc/UTC)) AS hour(lpep_pickup_datetime#13, Some(Etc/UTC))#59], functions=[partial_sum(total_amount#28)])\n",
      "                        +- *FileScan csv [lpep_pickup_datetime#13,total_amount#28] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/nbuser/library/green_tripdata_2017-06.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<lpep_pickup_datetime:timestamp,total_amount:double>\n"
     ]
    }
   ],
   "source": [
    "revenue_by_hour.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out revenue by hour via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# revenue_by_hour.write.csv(\"green_revenue_by_hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00022-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 20 Jan 24 20:11 green_revenue_by_hour/part-00006-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00003-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 20 Jan 24 20:11 green_revenue_by_hour/part-00016-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00020-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 20 Jan 24 20:11 green_revenue_by_hour/part-00008-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00014-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00015-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00007-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser  0 Jan 24 20:11 green_revenue_by_hour/part-00024-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00023-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00017-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 19 Jan 24 20:11 green_revenue_by_hour/part-00009-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00013-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00012-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00011-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 20 Jan 24 20:11 green_revenue_by_hour/part-00001-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 20 Jan 24 20:11 green_revenue_by_hour/part-00000-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00010-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00021-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00019-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 20 Jan 24 20:11 green_revenue_by_hour/part-00002-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00005-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00004-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n",
      "-rw-r--r-- 1 nbuser nbuser 21 Jan 24 20:11 green_revenue_by_hour/part-00018-2c744d1b-8d98-4de9-9fd8-55f0ddd1041d-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alrth green_revenue_by_hour/part*\n",
    "# 25 files, each with 1 line, and 1 file with 0 lines (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandas_rev_df = revenue_by_hour.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write revenue by hour to single CSV file via Pandas\n",
    "#### Note: Spark jobs are distributed and writing is done by each executor when using Spark's write API. Calling .toPandas() is _not_ free and will throw errors on larger datasets, because the *single driver machine* now has to hold the _entire dataset_ in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas_rev_df.to_csv(\"revenue_by_hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 nbuser nbuser 592 Jan 24 20:18 revenue_by_hour.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alrth revenue_by_hour.csv\n",
    "# Ahhhh, back to 1 file :phew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>ehail_fee</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-15 18:26:02</td>\n",
       "      <td>2017-06-15 18:28:10</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-30 20:40:36</td>\n",
       "      <td>2017-06-30 20:48:45</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>1.18</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-20 15:23:06</td>\n",
       "      <td>2017-06-20 16:04:21</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>4.55</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>30.36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-16 16:53:06</td>\n",
       "      <td>2017-06-16 17:00:59</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-06-18 19:36:42</td>\n",
       "      <td>2017-06-18 20:14:34</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>9.60</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>36.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime lpep_dropoff_datetime store_and_fwd_flag  \\\n",
       "0         2  2017-06-15 18:26:02   2017-06-15 18:28:10                  N   \n",
       "1         2  2017-06-30 20:40:36   2017-06-30 20:48:45                  N   \n",
       "2         2  2017-06-20 15:23:06   2017-06-20 16:04:21                  N   \n",
       "3         2  2017-06-16 16:53:06   2017-06-16 17:00:59                  N   \n",
       "4         1  2017-06-18 19:36:42   2017-06-18 20:14:34                  N   \n",
       "\n",
       "   RatecodeID  PULocationID  DOLocationID  passenger_count  trip_distance  \\\n",
       "0           1            80            80                1           0.43   \n",
       "1           1           112           255                1           1.18   \n",
       "2           1            40           255                1           4.55   \n",
       "3           1            95            95                1           0.77   \n",
       "4           1            75            33                1           9.60   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount ehail_fee  \\\n",
       "0          3.5    1.0      0.5        1.59           0.0      None   \n",
       "1          7.0    0.5      0.5        1.66           0.0      None   \n",
       "2         24.5    0.0      0.5        5.06           0.0      None   \n",
       "3          6.5    1.0      0.5        0.00           0.0      None   \n",
       "4         33.0    0.0      0.5        3.00           0.0      None   \n",
       "\n",
       "   improvement_surcharge  total_amount  payment_type  trip_type  \n",
       "0                    0.3          6.89             1          1  \n",
       "1                    0.3          9.96             1          1  \n",
       "2                    0.3         30.36             1          1  \n",
       "3                    0.3          8.30             2          1  \n",
       "4                    0.3         36.80             1          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(green_trips.sort(F.rand()).limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_trips.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_zones = spark.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .csv(\"taxi_zone_lookup.csv\")\n",
    "taxi_zones.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pu_taxi_zones = taxi_zones.toDF(\"PULocationID\", \"PUBorough\", \"PUZone\", \"PUservice_zone\") \n",
    "# select([F.col(c).alias(\"PU{c}\".format(c=c)) for c in taxi_zones.columns])\n",
    "green_trips_with_locations = green_trips.join(pu_taxi_zones, \"PULocationID\")\n",
    "\n",
    "do_taxi_zones = taxi_zones.toDF(\"DOLocationID\", \"DOBorough\", \"DOZone\", \"DOservice_zone\") \n",
    "# .select([F.col(c).alias(\"DO{c}\".format(c=c)) for c in taxi_zones.columns])\n",
    "green_trips_with_locations = green_trips_with_locations.join(do_taxi_zones, \"DOLocationID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>trip_type</th>\n",
       "      <th>PUBorough</th>\n",
       "      <th>PUZone</th>\n",
       "      <th>PUservice_zone</th>\n",
       "      <th>DOBorough</th>\n",
       "      <th>DOZone</th>\n",
       "      <th>DOservice_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-01 00:33:45</td>\n",
       "      <td>2017-06-01 01:39:52</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>90.41</td>\n",
       "      <td>404.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>413.51</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Brownsville</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NA</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-01 00:33:55</td>\n",
       "      <td>2017-06-01 23:36:23</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Yellow Zone</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>East Harlem South</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>255</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-01 00:12:07</td>\n",
       "      <td>2017-06-01 00:12:07</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.49</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40.38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Williamsburg (North Side)</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Central Park</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-01 00:17:26</td>\n",
       "      <td>2017-06-01 00:20:01</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Williamsburg (South Side)</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Williamsburg (South Side)</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135</td>\n",
       "      <td>130</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-01 00:17:22</td>\n",
       "      <td>2017-06-01 00:24:21</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.08</td>\n",
       "      <td>8.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica</td>\n",
       "      <td>Boro Zone</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Kew Gardens Hills</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOLocationID  PULocationID  VendorID lpep_pickup_datetime  \\\n",
       "0           265            35         2  2017-06-01 00:33:45   \n",
       "1            75           263         2  2017-06-01 00:33:55   \n",
       "2            43           255         2  2017-06-01 00:12:07   \n",
       "3           256           256         2  2017-06-01 00:17:26   \n",
       "4           135           130         2  2017-06-01 00:17:22   \n",
       "\n",
       "  lpep_dropoff_datetime store_and_fwd_flag  RatecodeID  passenger_count  \\\n",
       "0   2017-06-01 01:39:52                  N           4                1   \n",
       "1   2017-06-01 23:36:23                  N           1                1   \n",
       "2   2017-06-01 00:12:07                  N           1                1   \n",
       "3   2017-06-01 00:20:01                  N           1                1   \n",
       "4   2017-06-01 00:24:21                  N           1                3   \n",
       "\n",
       "   trip_distance  fare_amount      ...        improvement_surcharge  \\\n",
       "0          90.41        404.5      ...                          0.3   \n",
       "1           0.38          4.0      ...                          0.3   \n",
       "2           9.49         31.0      ...                          0.3   \n",
       "3           0.59          4.0      ...                          0.3   \n",
       "4           2.08          8.5      ...                          0.3   \n",
       "\n",
       "   total_amount  payment_type  trip_type  PUBorough  \\\n",
       "0        413.51             2          1   Brooklyn   \n",
       "1          6.89             1          1  Manhattan   \n",
       "2         40.38             1          1   Brooklyn   \n",
       "3          6.36             1          1   Brooklyn   \n",
       "4          9.80             2          1     Queens   \n",
       "\n",
       "                      PUZone  PUservice_zone  DOBorough  \\\n",
       "0                Brownsville       Boro Zone    Unknown   \n",
       "1             Yorkville West     Yellow Zone  Manhattan   \n",
       "2  Williamsburg (North Side)       Boro Zone  Manhattan   \n",
       "3  Williamsburg (South Side)       Boro Zone   Brooklyn   \n",
       "4                    Jamaica       Boro Zone     Queens   \n",
       "\n",
       "                      DOZone DOservice_zone  \n",
       "0                         NA            N/A  \n",
       "1          East Harlem South      Boro Zone  \n",
       "2               Central Park    Yellow Zone  \n",
       "3  Williamsburg (South Side)      Boro Zone  \n",
       "4          Kew Gardens Hills      Boro Zone  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(green_trips_with_locations.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "green_trips_with_locations.createOrReplaceTempView(\"green_trips_with_locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_net_fare</th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>972114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>4353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_net_fare  count(1)\n",
       "0         True    972114\n",
       "1        False      4353"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql_display\n",
    "SELECT total_amount > 0 AS is_net_fare, COUNT(1)\n",
    "FROM green_trips_with_locations\n",
    "GROUP BY total_amount > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quartiles by Total Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quartile_df = green_trips_with_locations.filter(\"total_amount > 0\")\\\n",
    "  .select(\n",
    "    # assign quartiles by total_amount    \n",
    "    F.ntile(4).over(Window.partitionBy().orderBy(\"total_amount\")).alias(\"quartile\"), \n",
    "    F.col(\"total_amount\"),\n",
    "    F.col(\"tip_amount\")\n",
    "  )\n",
    "# quartile_df.cache().count() # override lazy caching to eagerly cache by forcing the count action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quartile</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>21.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>16.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13.56</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>21.96</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>11.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>10.38</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>10.38</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>22.85</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quartile  total_amount  tip_amount\n",
       "0         4         21.30        0.00\n",
       "1         3         16.25        0.00\n",
       "2         3         13.56        2.26\n",
       "3         4         21.96        3.66\n",
       "4         3         11.80        0.00\n",
       "5         2         10.38        2.08\n",
       "6         2         10.38        2.08\n",
       "7         2         10.80        0.00\n",
       "8         3         13.30        0.00\n",
       "9         4         22.85        4.55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(quartile_df.sort(F.rand()).limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Revenue, Tips and % of Total Revenue by Quartile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quartile</th>\n",
       "      <th>avg_total</th>\n",
       "      <th>median_total</th>\n",
       "      <th>avg_tip</th>\n",
       "      <th>median_tip</th>\n",
       "      <th>avg_tip_pct</th>\n",
       "      <th>trips</th>\n",
       "      <th>revenue</th>\n",
       "      <th>rev_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.48</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.33</td>\n",
       "      <td>243029</td>\n",
       "      <td>1574406.33</td>\n",
       "      <td>11.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9.57</td>\n",
       "      <td>9.36</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.73</td>\n",
       "      <td>243029</td>\n",
       "      <td>2325976.00</td>\n",
       "      <td>16.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>13.91</td>\n",
       "      <td>13.80</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.96</td>\n",
       "      <td>243028</td>\n",
       "      <td>3381087.79</td>\n",
       "      <td>23.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28.40</td>\n",
       "      <td>24.35</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.96</td>\n",
       "      <td>9.67</td>\n",
       "      <td>243028</td>\n",
       "      <td>6902469.09</td>\n",
       "      <td>48.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quartile  avg_total  median_total  avg_tip  median_tip  avg_tip_pct  \\\n",
       "0         1       6.48          6.80     0.23        0.00         3.33   \n",
       "1         2       9.57          9.36     0.65        0.00         6.73   \n",
       "2         3      13.91         13.80     1.12        0.00         7.96   \n",
       "3         4      28.40         24.35     2.85        2.96         9.67   \n",
       "\n",
       "    trips     revenue  rev_pct  \n",
       "0  243029  1574406.33    11.10  \n",
       "1  243029  2325976.00    16.40  \n",
       "2  243028  3381087.79    23.84  \n",
       "3  243028  6902469.09    48.66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quartile_df_agg =  quartile_df.groupBy(\"quartile\").agg(\n",
    "  F.round(F.avg(\"total_amount\"),2).alias(\"avg_total\"), \n",
    "  F.round(F.expr(\"percentile(total_amount, 0.5)\"),2).alias(\"median_total\"), # percentile is a hive UDAF\n",
    "  F.round(F.avg(\"tip_amount\"),2).alias(\"avg_tip\"),\n",
    "  F.expr(\"percentile(tip_amount, 0.5)\").alias(\"median_tip\"), # percentile is a hive UDAF\n",
    "  F.round(F.avg(F.col(\"tip_amount\")/F.col(\"total_amount\"))*100,2).alias(\"avg_tip_pct\"),\n",
    "  F.count(F.lit(1)).alias(\"trips\"), \n",
    "  F.round(F.sum(\"total_amount\"),2).alias(\"revenue\")\n",
    ")\n",
    "total_revenue = quartile_df_agg.agg(F.sum(\"revenue\")).first()[\"sum(revenue)\"]\n",
    "display(quartile_df_agg.select(\"*\", F.round((F.col(\"revenue\")/F.lit(total_revenue))*100,2).alias(\"rev_pct\")).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_by_hour = green_trips_with_locations \\\n",
    ".filter(\"lpep_pickup_datetime LIKE '2017-06-27%'\") \\\n",
    ".groupBy(\n",
    "  F.window(\"lpep_pickup_datetime\", \"60 minutes\").getField(\"start\").alias(\"pickup_window\")\n",
    ")\n",
    "pickups_by_hour_pdf = grouped_by_hour \\\n",
    "  .count() \\\n",
    "  .withColumnRenamed(\"count\", \"hour_count\") \\\n",
    "  .sort(F.asc(\"pickup_window\")) \\\n",
    "  .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "2017-06-27",
          "2017-06-27 01:00:00",
          "2017-06-27 02:00:00",
          "2017-06-27 03:00:00",
          "2017-06-27 04:00:00",
          "2017-06-27 05:00:00",
          "2017-06-27 06:00:00",
          "2017-06-27 07:00:00",
          "2017-06-27 08:00:00",
          "2017-06-27 09:00:00",
          "2017-06-27 10:00:00",
          "2017-06-27 11:00:00",
          "2017-06-27 12:00:00",
          "2017-06-27 13:00:00",
          "2017-06-27 14:00:00",
          "2017-06-27 15:00:00",
          "2017-06-27 16:00:00",
          "2017-06-27 17:00:00",
          "2017-06-27 18:00:00",
          "2017-06-27 19:00:00",
          "2017-06-27 20:00:00",
          "2017-06-27 21:00:00",
          "2017-06-27 22:00:00",
          "2017-06-27 23:00:00"
         ],
         "y": [
          695,
          427,
          276,
          165,
          208,
          266,
          453,
          968,
          1622,
          1597,
          1661,
          1402,
          1457,
          1338,
          1453,
          1784,
          1793,
          2045,
          2307,
          2205,
          1690,
          1564,
          1456,
          1209
         ]
        }
       ],
       "layout": {
        "title": "Pickups on June 27, 2016 by Hour",
        "xaxis": {
         "title": "Hour of Day"
        },
        "yaxis": {
         "title": "Pickups"
        }
       }
      },
      "text/html": [
       "<div id=\"87630fec-6cb3-4980-8557-ffd91b13d558\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"87630fec-6cb3-4980-8557-ffd91b13d558\", [{\"type\": \"bar\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [695, 427, 276, 165, 208, 266, 453, 968, 1622, 1597, 1661, 1402, 1457, 1338, 1453, 1784, 1793, 2045, 2307, 2205, 1690, 1564, 1456, 1209]}], {\"title\": \"Pickups on June 27, 2016 by Hour\", \"xaxis\": {\"title\": \"Hour of Day\"}, \"yaxis\": {\"title\": \"Pickups\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"87630fec-6cb3-4980-8557-ffd91b13d558\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"87630fec-6cb3-4980-8557-ffd91b13d558\", [{\"type\": \"bar\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [695, 427, 276, 165, 208, 266, 453, 968, 1622, 1597, 1661, 1402, 1457, 1338, 1453, 1784, 1793, 2045, 2307, 2205, 1690, 1564, 1456, 1209]}], {\"title\": \"Pickups on June 27, 2016 by Hour\", \"xaxis\": {\"title\": \"Hour of Day\"}, \"yaxis\": {\"title\": \"Pickups\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.graph_objs import *\n",
    "\n",
    "data = [Bar(x=pickups_by_hour_pdf.pickup_window,\n",
    "            y=pickups_by_hour_pdf.hour_count)]\n",
    "\n",
    "layout = Layout(title=\"Pickups on June 27, 2016 by Hour\",\n",
    "                xaxis=dict(title='Hour of Day'),\n",
    "                yaxis=dict(title='Pickups'))\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='jupyter/basic_bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tip_pct_by_hour_pdf = grouped_by_hour.agg(\n",
    "    F.round(F.avg(F.col(\"tip_amount\")/F.col(\"total_amount\"))*100,2).alias(\"avg_tip_pct\")\n",
    "  )\\\n",
    "  .sort(F.asc(\"pickup_window\"))\\\n",
    "  .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "2017-06-27",
          "2017-06-27 01:00:00",
          "2017-06-27 02:00:00",
          "2017-06-27 03:00:00",
          "2017-06-27 04:00:00",
          "2017-06-27 05:00:00",
          "2017-06-27 06:00:00",
          "2017-06-27 07:00:00",
          "2017-06-27 08:00:00",
          "2017-06-27 09:00:00",
          "2017-06-27 10:00:00",
          "2017-06-27 11:00:00",
          "2017-06-27 12:00:00",
          "2017-06-27 13:00:00",
          "2017-06-27 14:00:00",
          "2017-06-27 15:00:00",
          "2017-06-27 16:00:00",
          "2017-06-27 17:00:00",
          "2017-06-27 18:00:00",
          "2017-06-27 19:00:00",
          "2017-06-27 20:00:00",
          "2017-06-27 21:00:00",
          "2017-06-27 22:00:00",
          "2017-06-27 23:00:00"
         ],
         "y": [
          6.09,
          5.17,
          5.2,
          5.08,
          4.54,
          6.74,
          6.96,
          7.63,
          7.6,
          7.9,
          8.05,
          6.67,
          5.93,
          5.92,
          5.96,
          5.85,
          6.32,
          6.71,
          7.76,
          7.54,
          7.54,
          8.15,
          8.26,
          7.76
         ]
        }
       ],
       "layout": {
        "title": "Avg Tip % on June 27, 2016 by Hour",
        "xaxis": {
         "title": "Hour of Day"
        },
        "yaxis": {
         "title": "Avg Tip %"
        }
       }
      },
      "text/html": [
       "<div id=\"77a2ef73-d8ad-48f9-b438-bbb8e79515fe\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"77a2ef73-d8ad-48f9-b438-bbb8e79515fe\", [{\"type\": \"bar\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [6.09, 5.17, 5.2, 5.08, 4.54, 6.74, 6.96, 7.63, 7.6, 7.9, 8.05, 6.67, 5.93, 5.92, 5.96, 5.85, 6.32, 6.71, 7.76, 7.54, 7.54, 8.15, 8.26, 7.76]}], {\"title\": \"Avg Tip % on June 27, 2016 by Hour\", \"xaxis\": {\"title\": \"Hour of Day\"}, \"yaxis\": {\"title\": \"Avg Tip %\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"77a2ef73-d8ad-48f9-b438-bbb8e79515fe\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"77a2ef73-d8ad-48f9-b438-bbb8e79515fe\", [{\"type\": \"bar\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [6.09, 5.17, 5.2, 5.08, 4.54, 6.74, 6.96, 7.63, 7.6, 7.9, 8.05, 6.67, 5.93, 5.92, 5.96, 5.85, 6.32, 6.71, 7.76, 7.54, 7.54, 8.15, 8.26, 7.76]}], {\"title\": \"Avg Tip % on June 27, 2016 by Hour\", \"xaxis\": {\"title\": \"Hour of Day\"}, \"yaxis\": {\"title\": \"Avg Tip %\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.graph_objs import *\n",
    "\n",
    "data = [Bar(x=tip_pct_by_hour_pdf.pickup_window,\n",
    "            y=tip_pct_by_hour_pdf.avg_tip_pct)]\n",
    "\n",
    "layout = Layout(title=\"Avg Tip % on June 27, 2016 by Hour\",\n",
    "                xaxis=dict(title='Hour of Day'),\n",
    "                yaxis=dict(title='Avg Tip %'))\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='jupyter/basic_bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_window</th>\n",
       "      <th>avg_tip_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06-27 00:00:00</td>\n",
       "      <td>6.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-06-27 01:00:00</td>\n",
       "      <td>5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-27 02:00:00</td>\n",
       "      <td>5.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06-27 03:00:00</td>\n",
       "      <td>5.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-06-27 04:00:00</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-06-27 05:00:00</td>\n",
       "      <td>6.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-06-27 06:00:00</td>\n",
       "      <td>6.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-06-27 07:00:00</td>\n",
       "      <td>7.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-06-27 08:00:00</td>\n",
       "      <td>7.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-06-27 09:00:00</td>\n",
       "      <td>7.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017-06-27 10:00:00</td>\n",
       "      <td>8.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017-06-27 11:00:00</td>\n",
       "      <td>6.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017-06-27 12:00:00</td>\n",
       "      <td>5.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-06-27 13:00:00</td>\n",
       "      <td>5.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-06-27 14:00:00</td>\n",
       "      <td>5.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-06-27 15:00:00</td>\n",
       "      <td>5.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-06-27 16:00:00</td>\n",
       "      <td>6.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-06-27 17:00:00</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017-06-27 18:00:00</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017-06-27 19:00:00</td>\n",
       "      <td>7.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017-06-27 20:00:00</td>\n",
       "      <td>7.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017-06-27 21:00:00</td>\n",
       "      <td>8.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017-06-27 22:00:00</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017-06-27 23:00:00</td>\n",
       "      <td>7.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pickup_window  avg_tip_pct\n",
       "0  2017-06-27 00:00:00         6.09\n",
       "1  2017-06-27 01:00:00         5.17\n",
       "2  2017-06-27 02:00:00         5.20\n",
       "3  2017-06-27 03:00:00         5.08\n",
       "4  2017-06-27 04:00:00         4.54\n",
       "5  2017-06-27 05:00:00         6.74\n",
       "6  2017-06-27 06:00:00         6.96\n",
       "7  2017-06-27 07:00:00         7.63\n",
       "8  2017-06-27 08:00:00         7.60\n",
       "9  2017-06-27 09:00:00         7.90\n",
       "10 2017-06-27 10:00:00         8.05\n",
       "11 2017-06-27 11:00:00         6.67\n",
       "12 2017-06-27 12:00:00         5.93\n",
       "13 2017-06-27 13:00:00         5.92\n",
       "14 2017-06-27 14:00:00         5.96\n",
       "15 2017-06-27 15:00:00         5.85\n",
       "16 2017-06-27 16:00:00         6.32\n",
       "17 2017-06-27 17:00:00         6.71\n",
       "18 2017-06-27 18:00:00         7.76\n",
       "19 2017-06-27 19:00:00         7.54\n",
       "20 2017-06-27 20:00:00         7.54\n",
       "21 2017-06-27 21:00:00         8.15\n",
       "22 2017-06-27 22:00:00         8.26\n",
       "23 2017-06-27 23:00:00         7.76"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_pct_by_hour_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "Pickups",
         "type": "bar",
         "x": [
          "2017-06-27",
          "2017-06-27 01:00:00",
          "2017-06-27 02:00:00",
          "2017-06-27 03:00:00",
          "2017-06-27 04:00:00",
          "2017-06-27 05:00:00",
          "2017-06-27 06:00:00",
          "2017-06-27 07:00:00",
          "2017-06-27 08:00:00",
          "2017-06-27 09:00:00",
          "2017-06-27 10:00:00",
          "2017-06-27 11:00:00",
          "2017-06-27 12:00:00",
          "2017-06-27 13:00:00",
          "2017-06-27 14:00:00",
          "2017-06-27 15:00:00",
          "2017-06-27 16:00:00",
          "2017-06-27 17:00:00",
          "2017-06-27 18:00:00",
          "2017-06-27 19:00:00",
          "2017-06-27 20:00:00",
          "2017-06-27 21:00:00",
          "2017-06-27 22:00:00",
          "2017-06-27 23:00:00"
         ],
         "y": [
          695,
          427,
          276,
          165,
          208,
          266,
          453,
          968,
          1622,
          1597,
          1661,
          1402,
          1457,
          1338,
          1453,
          1784,
          1793,
          2045,
          2307,
          2205,
          1690,
          1564,
          1456,
          1209
         ],
         "yaxis": "y2"
        },
        {
         "name": "Tip %",
         "type": "scatter",
         "x": [
          "2017-06-27",
          "2017-06-27 01:00:00",
          "2017-06-27 02:00:00",
          "2017-06-27 03:00:00",
          "2017-06-27 04:00:00",
          "2017-06-27 05:00:00",
          "2017-06-27 06:00:00",
          "2017-06-27 07:00:00",
          "2017-06-27 08:00:00",
          "2017-06-27 09:00:00",
          "2017-06-27 10:00:00",
          "2017-06-27 11:00:00",
          "2017-06-27 12:00:00",
          "2017-06-27 13:00:00",
          "2017-06-27 14:00:00",
          "2017-06-27 15:00:00",
          "2017-06-27 16:00:00",
          "2017-06-27 17:00:00",
          "2017-06-27 18:00:00",
          "2017-06-27 19:00:00",
          "2017-06-27 20:00:00",
          "2017-06-27 21:00:00",
          "2017-06-27 22:00:00",
          "2017-06-27 23:00:00"
         ],
         "y": [
          6.09,
          5.17,
          5.2,
          5.08,
          4.54,
          6.74,
          6.96,
          7.63,
          7.6,
          7.9,
          8.05,
          6.67,
          5.93,
          5.92,
          5.96,
          5.85,
          6.32,
          6.71,
          7.76,
          7.54,
          7.54,
          8.15,
          8.26,
          7.76
         ]
        }
       ],
       "layout": {
        "title": "Pickups and Tip % by Hour",
        "yaxis": {
         "title": "Tip %"
        },
        "yaxis2": {
         "overlaying": "y",
         "side": "right",
         "tickfont": {
          "color": "rgb(148, 103, 189)"
         },
         "title": "Pickups",
         "titlefont": {
          "color": "rgb(148, 103, 189)"
         }
        }
       }
      },
      "text/html": [
       "<div id=\"1fd4b773-a7ee-4c96-a0e3-59cc2484f4a1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1fd4b773-a7ee-4c96-a0e3-59cc2484f4a1\", [{\"type\": \"bar\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [695, 427, 276, 165, 208, 266, 453, 968, 1622, 1597, 1661, 1402, 1457, 1338, 1453, 1784, 1793, 2045, 2307, 2205, 1690, 1564, 1456, 1209], \"name\": \"Pickups\", \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [6.09, 5.17, 5.2, 5.08, 4.54, 6.74, 6.96, 7.63, 7.6, 7.9, 8.05, 6.67, 5.93, 5.92, 5.96, 5.85, 6.32, 6.71, 7.76, 7.54, 7.54, 8.15, 8.26, 7.76], \"name\": \"Tip %\"}], {\"title\": \"Pickups and Tip % by Hour\", \"yaxis\": {\"title\": \"Tip %\"}, \"yaxis2\": {\"title\": \"Pickups\", \"titlefont\": {\"color\": \"rgb(148, 103, 189)\"}, \"tickfont\": {\"color\": \"rgb(148, 103, 189)\"}, \"overlaying\": \"y\", \"side\": \"right\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"1fd4b773-a7ee-4c96-a0e3-59cc2484f4a1\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"1fd4b773-a7ee-4c96-a0e3-59cc2484f4a1\", [{\"type\": \"bar\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [695, 427, 276, 165, 208, 266, 453, 968, 1622, 1597, 1661, 1402, 1457, 1338, 1453, 1784, 1793, 2045, 2307, 2205, 1690, 1564, 1456, 1209], \"name\": \"Pickups\", \"yaxis\": \"y2\"}, {\"type\": \"scatter\", \"x\": [\"2017-06-27\", \"2017-06-27 01:00:00\", \"2017-06-27 02:00:00\", \"2017-06-27 03:00:00\", \"2017-06-27 04:00:00\", \"2017-06-27 05:00:00\", \"2017-06-27 06:00:00\", \"2017-06-27 07:00:00\", \"2017-06-27 08:00:00\", \"2017-06-27 09:00:00\", \"2017-06-27 10:00:00\", \"2017-06-27 11:00:00\", \"2017-06-27 12:00:00\", \"2017-06-27 13:00:00\", \"2017-06-27 14:00:00\", \"2017-06-27 15:00:00\", \"2017-06-27 16:00:00\", \"2017-06-27 17:00:00\", \"2017-06-27 18:00:00\", \"2017-06-27 19:00:00\", \"2017-06-27 20:00:00\", \"2017-06-27 21:00:00\", \"2017-06-27 22:00:00\", \"2017-06-27 23:00:00\"], \"y\": [6.09, 5.17, 5.2, 5.08, 4.54, 6.74, 6.96, 7.63, 7.6, 7.9, 8.05, 6.67, 5.93, 5.92, 5.96, 5.85, 6.32, 6.71, 7.76, 7.54, 7.54, 8.15, 8.26, 7.76], \"name\": \"Tip %\"}], {\"title\": \"Pickups and Tip % by Hour\", \"yaxis\": {\"title\": \"Tip %\"}, \"yaxis2\": {\"title\": \"Pickups\", \"titlefont\": {\"color\": \"rgb(148, 103, 189)\"}, \"tickfont\": {\"color\": \"rgb(148, 103, 189)\"}, \"overlaying\": \"y\", \"side\": \"right\"}}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trace1 = Bar(x=pickups_by_hour_pdf.pickup_window,\n",
    "            y=pickups_by_hour_pdf.hour_count,\n",
    "            name=\"Pickups\",\n",
    "            yaxis='y2')\n",
    "trace2 = Scatter(x=tip_pct_by_hour_pdf.pickup_window,\n",
    "            y=tip_pct_by_hour_pdf.avg_tip_pct,\n",
    "            name=\"Tip %\")\n",
    "data = [trace1, trace2]\n",
    "layout = Layout(\n",
    "    title='Pickups and Tip % by Hour',\n",
    "    yaxis=dict(\n",
    "        title='Tip %'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Pickups',\n",
    "        titlefont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    )\n",
    ")\n",
    "fig = Figure(data=data, layout=layout)\n",
    "plot_url = iplot(fig, filename='multiple-axes-double')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accumulator Rows: 24\n",
      "DF Row Count: 24\n"
     ]
    }
   ],
   "source": [
    "rows_accum = spark.sparkContext.accumulator(0)\n",
    "revenue_by_hour.foreach(lambda r: rows_accum.add(1)) # rowsAccum.add(1))\n",
    "print(\"Accumulator Rows: {ar}\".format(ar=rows_accum.value))\n",
    "print(\"DF Row Count: {c}\".format(c=revenue_by_hour.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-specific Advice\n",
    "#### My blog post [here](http://garrens.com/blog/2018/01/06/scaling-python-for-data-science-using-spark/) covers the topic of scaling Python for Spark with examples and thorough explanation\n",
    "\n",
    "#### Avoid Python-based UDFs - instead using *PySpark* APIs like pyspark.sql.functions, which utilize the JVM for high performance.  \n",
    "\n",
    "Note: in this notebook, the performance doesn't appear to be much different, but in many other circumstances, expect the performance hit from using python APIs in Spark to be more pronounced. With Spark 2.3 coming out, vectorized UDFs will be supported for Python which will greatly improve performance. See [this post](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 ms, sys: 12.3 ms, total: 30.4 ms\n",
      "Wall time: 36.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "timestamp_to_epoch = F.udf(lambda t: int(t.strftime(\"%s\")))\n",
    "green_trips_with_locations.select(\n",
    "    timestamp_to_epoch(green_trips_with_locations.lpep_pickup_datetime),\n",
    "    F.col(\"DOBorough\"),\n",
    "    F.col(\"DOZone\")\n",
    "    ).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970934\n",
      "CPU times: user 853 µs, sys: 10.3 ms, total: 11.1 ms\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(green_trips_with_locations.select(\n",
    "    F.unix_timestamp(green_trips_with_locations.lpep_pickup_datetime),\n",
    "    F.col(\"DOBorough\"),\n",
    "    F.col(\"DOZone\")\n",
    ").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Be cautious when using [your favorite Python packages here] with Spark\n",
    "\n",
    "Python package code (such as pandas, numpy, etc) executes on one machine (the driver) using 1 core while Spark is distributed. \n",
    "\n",
    "You probably **can** use any package you like, but just because you can **doesn’t mean you should**.\n",
    "\n",
    "#### Friends don't let friends use python packages with spark unwittingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No, don’t use RDDs; use [Spark] SQL!\n",
    "\n",
    "RDDs are the foundational data structure of Spark, but they are not optimized. DataFrames and all other SQL functionality is automatically optimized through Catalyst and Tungsten. Python is exceptionally slow when using RDDs due to the [de]serialization and python process overhead compared to the DataFrame APIs.\n",
    "\n",
    "![Spark SQL](Screen Shot 2018-01-24 at 11.22.49 AM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/nbuser/library/i_dont_exist.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o218.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/nbuser/library/i_dont_exist.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:626)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:533)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0d9c9e560428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i_dont_exist.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Py4JJavaError: An error occurred while calling o33.csv.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# : org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/nbuser/library/i_dont_exist.csv;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/nbuser/library/i_dont_exist.csv;'"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"i_dont_exist.csv\")\n",
    "\n",
    "# Py4JJavaError: An error occurred while calling o33.csv.\n",
    "# : org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/nbuser/library/i_dont_exist.csv;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truck loads of small files = death by 1,000 very slow cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = spark.read.csv(\"small_files/\")\n",
    "df.count()\n",
    "\n",
    "# CPU times: user 13.9 ms, sys: 1.33 ms, total: 15.2 ms\n",
    "# ** Wall time: 1min 7s**\n",
    "# mkdir small_files\n",
    "# split -l 2000 green_tripdata_2017-06.csv small_files/\n",
    "# ~500 files 150-200KB each :(\n",
    "# http://garrens.com/blog/2017/11/04/big-data-spark-and-its-small-files-problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GZip is _not splittable_ therefore cannot be parallelized across cluster - re: 1 unsplittable file = 1 partition\n",
    "options: \n",
    "\n",
    "1) don't use GZIP\n",
    "\n",
    "2) call repartition(num_partitions) immediately after reading in file(s), which will shuffle the data across the network to each executor roughly equally. Expensive network-wise, but worthwhile if you're accessing the data frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = spark.read.csv(\"green_tripdata_2017-06.csv.gz\")\n",
    "\n",
    "# CPU times: user 2.08 ms, sys: 756 µs, total: 2.83 ms\n",
    "# Wall time: 289 ms\n",
    "\n",
    "# cat green_tripdata_2017-06.csven_tripdata_2017-06.csv | gzip --fast > green_tripdata_2017-06.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most basic incarnation of reading CSV file without header OR inferred Schema, therefore every column has a generic name and is of String type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.93 ms, sys: 2.06 ms, total: 8.99 ms\n",
      "Wall time: 33.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(\"green_tripdata_2017-06.csv\")\n",
    "df.count()\n",
    "# CPU times: user 1.56 ms, sys: 566 µs, total: 2.12 ms\n",
    "# Wall time: ** 259 ms **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
